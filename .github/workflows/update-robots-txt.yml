name: Update robots.txt

on:
  schedule:
    - cron: '0 0 * * 0'
  workflow_dispatch:

jobs:
  update-robots:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Update robot.txt
        id: update
        env:
          API_KEY: ${{ secrets.ROBOTS }}
        run: |
          cp robots-base.txt public/robots.txt
          curl --location 'https://api.darkvisitors.com/robots-txts' \
          --header 'Content-Type: application/json' \
          --header "Authorization: Bearer $API_KEY" \
          --data '{ "agent_types": [ "AI Data Scraper", "AI Assistant", "AI Search Crawler", "Undocumented AI Agent" ], "disallow": "/" }' >> public/robots.txt

      - name: Create pull request
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: robots.txt-update
          title: 'Update robots.txt'
          commit-message: 'Update robots.txt'
          labels: 'robots.txt'
          add-paths: public/robots.txt
          body: This PR was generated by the `Update robots.txt` action and contains updates to our robots.txt file, pulled from [Dark Visitors](https://darkvisitors.com/).
